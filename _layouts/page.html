---
layout: default
---
<!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">{{ page.title }}</h1>
            <p class="post-description">{{ page.description }}</p>
          </header>

          <article>
            {{ content }}
          </article>

          {%- if page.related_publications != null and page.related_publications.size > 0 -%}
          {% assign publications = page.related_publications | replace: ", ", "," | split: "," | join: "|" %}
          <h2>References</h2>
          <div class="publications">
              <h2 class="year">2020</h2>
              <ol class="bibliography"><li>
            <!-- _layouts/bib.html -->
                  <div class="row">
                    <div class="col-sm-2 abbr"></div>
            
                    <!-- Entry bib key -->
                    <div id="a" class="col-sm-8">
                    
                      <!-- Title -->
                      <div class="title">Reinvestigating the Merger of Mid-Front Vowels in Seoul Korean</div>
                      <!-- Author -->
                      <div class="author">
                      Woo Jin Choi
            </div>
            
                      <!-- Journal/Book title and date -->
                      
                      <div class="periodical">
                        <em></em>
                      </div>
                    
                      <!-- Links/Buttons -->
                      <div class="links">
                        <a href="/assets/pdf/Reinvestigating_the_Merger_of_Mid_Front_Vowels_in_Seoul_Korean.pdf.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
                      </div>
            
                      <!-- Hidden abstract block -->
            <!--           <div class="abstract hidden">
                        <p>Speech recognition and machine translation have made major progress over the past decades, providing practical systems to map one language sequence to another. Although multiple modalities such as sound and video are becoming increasingly available, the state-of-the-art systems are inherently unimodal, in the sense that they take a single modality - either speech or text - as input. Evidence from human learning suggests that additional modalities can provide disambiguating signals crucial for many language tasks. In this article, we describe the How2 dataset, a large, open-domain collection of videos with transcriptions and their translations. We then show how this single dataset can be used to develop systems for a variety of language tasks and present a number of models meant as starting points. Across tasks, we find that building multimodal architectures that perform better than their unimodal counterpart remains a challenge. This leaves plenty of room for the exploration of more advanced solutions that fully exploit the multimodal nature of the How2 dataset, and the general direction of multimodal learning with other datasets as well.</p>
                      </div> -->
                    </div>
                  </div>
            </li></ol>
          </div>
          {%- endif %}

          {%- if site.giscus.repo and page.giscus_comments -%}
            {% include giscus.html %}
          {%- endif -%}
        </div>
